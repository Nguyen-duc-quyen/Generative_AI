{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "\n",
    "import sys\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "\n",
    "#from torchvision import transforms\n",
    "from torchvision.transforms import v2 # for torchvision > 0.15\n",
    "import torch.nn as nn\n",
    "import torcheval.metrics as metrics\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.dataloaders import *\n",
    "from utils.preprocessing import *\n",
    "from utils.training_utils import *\n",
    "from utils.metrics import *\n",
    "from utils.losses import *\n",
    "\n",
    "from autoencoder.models import *\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = idx2numpy.convert_from_file(os.path.join(data_dir, \"train\", \"train-images-idx3-ubyte\"))\n",
    "train_labels = idx2numpy.convert_from_file(os.path.join(data_dir, \"train\", \"train-labels-idx1-ubyte\"))\n",
    "test_images = idx2numpy.convert_from_file(os.path.join(data_dir, \"test\", \"t10k-images-idx3-ubyte\"))\n",
    "test_labels = idx2numpy.convert_from_file(os.path.join(data_dir, \"test\", \"t10k-labels-idx1-ubyte\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Training set shape: (60000, 28, 28)\n",
      "[INFO]: Test set shape: (10000, 28, 28)\n",
      "[INFO]: Training label shape: (60000,)\n",
      "[INFO]: Test label shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Data stats\n",
    "print(\"[INFO]: Training set shape: {}\".format(train_images.shape))\n",
    "print(\"[INFO]: Test set shape: {}\".format(test_images.shape))\n",
    "print(\"[INFO]: Training label shape: {}\".format(train_labels.shape))\n",
    "print(\"[INFO]: Test label shape: {}\".format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image scaling\n",
    "train_images = train_images/255\n",
    "test_images = test_images/255\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = np.mean(train_images)\n",
    "std = np.std(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_transform = v2.Compose([\n",
    "    v2.ToTensor(), # deprecated # Will transpose the image, since torch expected the input image to be of shape [H, W, C] -> [C, H, W]\n",
    "    v2.Normalize(mean=[mean], std=[std]),\n",
    "    v2.ToDtype(torch.float32, scale=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MnistDataset(images=train_images, labels=train_labels, transforms=train_transform)\n",
    "test_dataset = MnistDataset(images=test_images, labels=test_labels, transforms=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "in_channels = 1\n",
    "enc_hidden_channels = 16\n",
    "emb_channels = 4\n",
    "dec_hidden_channels = 16\n",
    "\n",
    "# Training configuration\n",
    "epochs = 25\n",
    "batchsize = 128\n",
    "learning_rate = 0.001\n",
    "num_workers = 4\n",
    "device = \"cuda\"\n",
    "use_wandb = True\n",
    "use_tensorboard = True\n",
    "\n",
    "# Checkpoint dir\n",
    "save_dir = \".\\checkpoints\"\n",
    "exp_name = \"conv_autoencoder_{}\".format(str(date.today()))\n",
    "save_dir = os.path.join(save_dir, exp_name)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.system(\"mkdir {}\".format(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loader(train_dataset, batchsize, num_workers, shuffle=True, drop_last=False)\n",
    "test_loader = get_loader(test_dataset, batchsize, num_workers, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv_AutoEncoder(in_channels, enc_hidden_channels, emb_channels, dec_hidden_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    # The first thre metrics only work on images\n",
    "    # metrics.FrechetInceptionDistance(device=device), # Only work on three-channel images\n",
    "    # metrics.StructuralSimilarity(device=device),\n",
    "    metrics.PeakSignalNoiseRatio(device=device),\n",
    "    # metrics.MeanSquaredError(device=device)\n",
    "]\n",
    "metric_weights = [1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquyenobest2000\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Code\\Generative_AI\\Generative_AI\\autoencoder\\wandb\\run-20240722_084924-mcil38yp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quyenobest2000/conv-mnist-autoencoder/runs/mcil38yp' target=\"_blank\">conv_autoencoder_2024-07-22</a></strong> to <a href='https://wandb.ai/quyenobest2000/conv-mnist-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quyenobest2000/conv-mnist-autoencoder' target=\"_blank\">https://wandb.ai/quyenobest2000/conv-mnist-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quyenobest2000/conv-mnist-autoencoder/runs/mcil38yp' target=\"_blank\">https://wandb.ai/quyenobest2000/conv-mnist-autoencoder/runs/mcil38yp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb:\n",
    "    # initialize wandb for usage\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"conv-mnist-autoencoder\", \n",
    "        # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "        name=exp_name, \n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"architecture\": str(model.__class__.__name__),\n",
    "            \"dataset\": \"MNIST\",\n",
    "            \"epochs\": epochs,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             160\n",
      "         MaxPool2d-2           [-1, 16, 14, 14]               0\n",
      "            Conv2d-3            [-1, 4, 14, 14]             580\n",
      "         MaxPool2d-4              [-1, 4, 7, 7]               0\n",
      "      Conv_Encoder-5              [-1, 4, 7, 7]               0\n",
      "   ConvTranspose2d-6           [-1, 16, 14, 14]             272\n",
      "   ConvTranspose2d-7            [-1, 1, 28, 28]              65\n",
      "      Conv_Decoder-8            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 1,077\n",
      "Trainable params: 1,077\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.16\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n",
      "[INFO]: Epoch: 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The dimension `input` and `target` should be 1D or 2D, got shapes torch.Size([128, 1, 28, 28]) and torch.Size([128, 1, 28, 28]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_tensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Code\\Generative_AI\\Generative_AI\\utils\\training_utils.py:229\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[1;34m(model, train_loader, val_loader, optimizer, loss_func, metrics, metric_weights, device, num_epochs, log_rate, save_rate, save_dir, logging_level, use_wandb, use_tensorboard, resume_training, checkpoint_path, interval, lr_scheduler)\u001b[0m\n\u001b[0;32m    227\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    228\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO]: Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs))\n\u001b[1;32m--> 229\u001b[0m train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Logging using logger\u001b[39;00m\n\u001b[0;32m    232\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO]: Training Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Code\\Generative_AI\\Generative_AI\\utils\\training_utils.py:95\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, loss_func, metrics, device)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m---> 95\u001b[0m         \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Calculate the final metrices\u001b[39;00m\n\u001b[0;32m     98\u001b[0m metrics_res \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torcheval\\metrics\\regression\\mean_squared_error.py:107\u001b[0m, in \u001b[0;36mMeanSquaredError.update\u001b[1;34m(self, input, target, sample_weight)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# pyre-ignore[14]: inconsistent override on *_:Any, **__:Any\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m     sample_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     94\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TMeanSquaredError:\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    Update states with the ground truth values and predictions.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m            Tensor of sample weights with shape of (n_sample, ). Defaults to None.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     (\n\u001b[0;32m    105\u001b[0m         sum_squared_error,\n\u001b[0;32m    106\u001b[0m         sum_weight,\n\u001b[1;32m--> 107\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_mean_squared_error_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_squared_error\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sum_squared_error\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_squared_error \u001b[38;5;241m=\u001b[39m sum_squared_error\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torcheval\\metrics\\functional\\regression\\mean_squared_error.py:77\u001b[0m, in \u001b[0;36m_mean_squared_error_update\u001b[1;34m(input, target, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean_squared_error_update\u001b[39m(\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     74\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     75\u001b[0m     sample_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m     76\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m---> 77\u001b[0m     \u001b[43m_mean_squared_error_update_input_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _update(\u001b[38;5;28minput\u001b[39m, target, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torcheval\\metrics\\functional\\regression\\mean_squared_error.py:120\u001b[0m, in \u001b[0;36m_mean_squared_error_update_input_check\u001b[1;34m(input, target, sample_weight)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean_squared_error_update_input_check\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    116\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    117\u001b[0m     sample_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m target\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    121\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dimension `input` and `target` should be 1D or 2D, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         )\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    126\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `input` and `target` should have the same size, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The dimension `input` and `target` should be 1D or 2D, got shapes torch.Size([128, 1, 28, 28]) and torch.Size([128, 1, 28, 28])."
     ]
    }
   ],
   "source": [
    "train_epochs(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=loss_func,\n",
    "    metrics=metrics,\n",
    "    metric_weights=metric_weights,\n",
    "    device=device,\n",
    "    num_epochs=epochs,\n",
    "    log_rate=5,\n",
    "    save_rate=10,\n",
    "    save_dir=save_dir,\n",
    "    use_tensorboard=use_tensorboard,\n",
    "    use_wandb=use_wandb,\n",
    "    interval=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
